I"/<h2 id="dataset">Dataset</h2>
<p>I used two public datasets <strong>MIMIC CXR</strong> and <strong>CheXpert</strong>. I created the prompts myself based on the labels.</p>

<hr />
<h2 id="normal-stable-diffusion">Normal Stable Diffusion.</h2>

<p>I tried to use the normal stable diffusion model to keep training, the result was really terrible. The images looked like some chest x-rays but as we all know human’s ribs didn’t look like that. Then I realized that the model was always keeping finding to best tokens to regenerate your images input, but since the original dataset they used didn’t have so many medical images, it was impossible for the model to generate the images we wanted.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/tian/assets/img/stable_diffusion/Normal_stable_diffusion_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<hr />
<h2 id="dreambooth">DreamBooth</h2>

<p>After 20000 training iterations, you could see from the images below that most of the generated images were laterals. As long as the machine could tell the difference between frontal and lateral images, it could also tell the difference between different diseases then.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/tian/assets/img/stable_diffusion/Different_iterations.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>These are the images generated with different diseases’ prompts:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/tian/assets/img/stable_diffusion/Different_diseases_2000_iterations.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>I also tried the eyeball dataset as well, but I couldn’t tell the difference between the RG and NRG myself. :joy:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/tian/assets/img/stable_diffusion/RG_NRG.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<hr />
<h2 id="the-main-difference-in-code">The main difference in code</h2>
<p>The main difference inside the code is that <strong>Stable Diffusion</strong> trained the text_encoder but <strong>DreamBooth</strong> trained the unet part.</p>

<h3 id="stable-diffusion">Stable diffusion</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Move vae and Unet to device
</span><span class="n">vae</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">unet</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Keep vae and Unet in eval model as we don't train these
</span><span class="n">vae</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">unet</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="o">-------------------------</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_train_epochs</span><span class="p">):</span>
    <span class="n">text_encoder</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="p">...</span>
</code></pre></div></div>

<h3 id="dreambooth-1">DreamBooth</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Move text_encoder and vae to gpu
</span><span class="n">text_encoder</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">vae</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="o">-------------------------</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_train_epochs</span><span class="p">):</span>
    <span class="n">unet</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="p">...</span>
</code></pre></div></div>

<hr />
<h2 id="auc-of-generated-images">AUC of Generated Images</h2>

<p>I wanted to use the generated images to build a dataset and compared the classification performance between it and the original dataset. The accuracy of generated images were all higher compared with the original-images but the mean AUC was 10 percent lower. Only the disease Edema’s AUC was higher.</p>

<table border="1">
 <colgroup>
    <col width="150" />
    <col width="150" />
    <col width="150" />
    <col width="150" />
    <col width="150" />
    <col width="150" />
 </colgroup>
 <tr> 
    <th>AUC (Best)</th>
    <th>Cardiomegaly</th>
    <th>Edema</th>
    <th>Consolidation</th>
    <th>Atelectasis</th>
    <th>Pleural Effusion</th>
  </tr>
  <tr>
    <td>Original images</td>
    <td>0.737</td>
    <td>0.8449</td>
    <td>0.8021</td>
    <td>0.7603</td>
    <td>0.7967</td>
  </tr>
  <tr> 
    <td>50000 iterations</td>
    <td>0.6215</td>
    <td>0.8754</td>
    <td>0.5716</td>
    <td>0.405 (Lowest)</td>
    <td>0.7435</td>
 </tr>
 <tr> 
    <td>30000 iterations</td>
    <td>0.6167</td>
    <td>0.88</td>
    <td>0.2762 (Lowest)</td>
    <td>0.2849 (Lowest)</td>
    <td>0.8169</td>
 </tr>
</table>
:ET